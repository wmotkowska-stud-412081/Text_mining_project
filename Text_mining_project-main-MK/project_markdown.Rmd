---
title: 'Kardashian Show: Sentiment Analysis'
author: "Weronika Motkowska, Magdalena Kowalewska"
date: "2023-02-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

### Purpose of the prject
This R project aims at enabling efficient sentimental analysis of episodes of "Keeping up with the Kardashians". The show has 20 seasons each with many episodes. 
In the project:<br />
1. extend of existence of each sentiment will be evaluated<br /> 
2. sentiment of the whole episode will be established<br />
3. multiple sentiment dictionaries will be compared <br />

### Main assumptions 

1. <br />
2. <br />



### Loading needed libraries

```{r libraries, echo=T, results='hide', message=FALSE}
library(tidyverse)
library(tm)
library(SnowballC)
library(tidytext)
library(textdata)
library(ggplot2)
library(wordcloud)
library(reshape2)
```

## Data 

### Description of the data set
The shows transcript can be found on subslikescript website: https://subslikescript.com/series/Keeping_Up_with_the_Kardashians-1086761#google_vignette. There is a link to each of the episodes from all seasons. The data is a .txt file, containing a set of conversations, which were conveyed in the episode. The text contains elements such as punctuation, blank spaces and stop words. 

### Preparation of data for modeling
The below project was performed using the second episode of the nineteenth season, but it can be replicated using any transcripts from any of the episodes. To prepare the date two functions were created. <br />
<br />
The result of the first function is a data frame with words and assigned to them their frequencies. Frequencies are defined as the number of occurrence of each word. The data frame is filtered to show only words which have occurred more than six times. White performing the data cleaning "like" was identified as a stop word. <br />
<br />
The second function is similar but provides all words with sentiment in the order, in which they occur in the passage. The result is a table with words, frequencies and order id. 

#### First function: kards.show.analysis

```{r echo=T, message=FALSE, warning=FALSE, results='hide'}
kards.show.analysis <- function(name) {
    text <- readLines(name)
    text
    
    name <- str_sub(name, start = 1, end =  -5)
    
    # Cleaning the transcript
    
    docs <- Corpus(VectorSource(text))
    toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
    docs <- tm_map(docs, stripWhitespace)
    docs <- tm_map(docs, removePunctuation)
    docs <- tm_map(docs, removeNumbers)
    docs <- tm_map(docs, content_transformer(tolower))
    inspect(docs)
    
    # We suspect "like" to be a stop word for Kardashians
    
    setdiff("so", stopwords("english"))
    setdiff("like", stopwords("english"))
    
    docs <- tm_map(docs, removeWords, stopwords("english"))

    
    dtm <- TermDocumentMatrix(docs) %>% 
        as.matrix() %>% 
        rowSums() %>% 
        data.frame(word = names(.), freq = .) %>% 
        arrange(desc(freq)) %>% 
        filter(freq > 10)
    
    head(dtm)
    
    # Kardashians mostly use their own stopping words in text
    docs <- tm_map(docs, removeWords, c("like", "yeah", "dont", "just", "know", "god", "really", "gonna", "get", "got",      "right", "can", "cant"))
    
    dtm <- TermDocumentMatrix(docs) %>%
        as.matrix() %>%
        rowSums() %>%
        data.frame(word = names(.), freq = .) %>%
        arrange(desc(freq)) %>% 
        filter(freq > 6)
    
    dtm1 <- dtm
    
    rownames(dtm) <- NULL
    
    head(dtm, 25)
    
    # Sentiment Analysis - automated process of understanding of the emotional intent of words to infer whether a      section of text is positive, negative or neutral
    
    get_sentiments("nrc") %>% distinct(sentiment)
    
    get_sentiments("nrc") %>% 
        filter(sentiment == "joy") %>%
        inner_join(dtm)
    
    get_sentiments("nrc") %>% 
        filter(sentiment == "positive") %>%
        inner_join(dtm)
    
    get_sentiments("nrc") %>% 
        filter(sentiment == "negative") %>%
        inner_join(dtm)
    
    name
    
    # Transcript with Kardashian stopping words (like, you know)
    
    # dtm1 %>%
    #   with(wordcloud(word, freq, max.words = 25, scale = c(1, 10)))
    
    
    
    # Transcript wo Kardashian stopping words (like, you know)
    
    return(dtm)
    
    
    # Word association a form of analyzing the content of text data in search of relations between terms
    
    # Keyword Extraction - the most relevant terms within a text, terms that summarize the contents of text in list         form, keyword extraction can be used to index data to be searched and to generate word clouds (a visual         representation of text data)

}



```

```{r echo=T, message=FALSE, warning=FALSE, results='hide' }
name <- "kuwtk_se19e02.txt"
name1 <- str_sub(name, start = 1, end =  -5)
dtm <- kards.show.analysis(name)

```



```{r echo=T}

summary(dtm)
```


#### Second function: kards.show.analysis.all

```{r echo=T}
kards.show.analysis.all <- function(name) {
    text <- readLines(name)
    text
    
    name <- str_sub(name, start = 1, end =  -5)
    
    # Cleaning the transcript
    
    docs <- Corpus(VectorSource(text))
    toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
    docs <- tm_map(docs, stripWhitespace)
    docs <- tm_map(docs, removePunctuation)
    docs <- tm_map(docs, removeNumbers)
    docs <- tm_map(docs, content_transformer(tolower))
    inspect(docs)
    
    
    setdiff("so", stopwords("english"))
    setdiff("like", stopwords("english"))
    docs <- tm_map(docs, removeWords, stopwords("english"))
    docs <- tm_map(docs, removeWords, c("like", "yeah", "dont", "just", "know", "god", "really", "gonna", "get", "got",      "right", "can", "cant"))
    
    
    
    dtm <- TermDocumentMatrix(docs) %>%
        as.matrix() %>% 
        rowSums() %>%
        data.frame(word = names(.), freq = .) %>%
        mutate(order = row_number())
    
    dtm1 <- dtm
    
    rownames(dtm) <- NULL
    
    head(dtm, 25)
    
    # Sentiment Analysis - automated process of understanding of the emotional intent of words to infer whether a      section of text is positive, negative or neutral
    
    get_sentiments("nrc") %>% distinct(sentiment)
    
    get_sentiments("nrc") %>% 
        filter(sentiment == "joy") %>%
        inner_join(dtm)
    
    get_sentiments("nrc") %>% 
        filter(sentiment == "positive") %>%
        inner_join(dtm)
    
    get_sentiments("nrc") %>% 
        filter(sentiment == "negative") %>%
        inner_join(dtm)
    
    name
    
    return(dtm)
    
}
```

```{r echo=T, message=FALSE, warning=FALSE, results='hide' }
dtm <- kards.show.analysis.all(name)    

```


```{r echo=T}

summary(dtm)
```


## Modeling
To model the data, firstly the kards.show.analysis function was applied. 

```{r echo=T, message=FALSE, warning=FALSE, results='hide' }
dtm <- kards.show.analysis(name) 

```

Then  sentiment was assigned to the results previously ran function. 

```{r echo=T}


sentiment <- get_sentiments("nrc") %>% 
    right_join(dtm) %>% 
    na.omit() %>% 
    group_by(sentiment) %>% 
    arrange(desc(freq)) %>% 
    ungroup()

pdf(file = paste0(name1, ".pdf"),
    title = name,
    width = 15,
    height = 7)

```

Below are displayed most frequent words in the episode

```{r echo=T}

dtm %>%
    with(wordcloud(word, freq, max.words = 25, scale = c(0.1, 3)))

```

Below the difference in words assigned to sentiment can be observed. Two dictionaries were compared: Bing and NRC.Seven words are in both dictionaries. 



```{r echo=T}

dtm %>% inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>% acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(color = c("#fc413a","#2da612"), max.words = 25)


```

```{r echo=T}


dtm %>% inner_join(get_sentiments("nrc") %>% filter(sentiment %in% c("positive", "negative"))) %>%
  count(word, sentiment, sort = TRUE) %>% acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(color = c("#fc413a","#2da612"), max.words = 25)

```

In the NRC there are more sentiment categorizes than only "positive" and "negative". Hence, one word even with only positive connotations may have multiple sentiments assigned. In this project the sentiment of most frequent words were examined.


```{r echo=T}

ggplot(sentiment, aes(sentiment, freq, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~word, ncol = 2) +
    labs(title = "Most frequent words their usage and sentiment")
```

The below graph represents the summary of all sentiments in the episode. 

The word frequency shows that the episode include some kind of prank on one of the family members, there is some talk about coronavirus so the episode can be set in time without knowledge of airing time. We see usage of "kim", "kris" who is "mom".  Most of the emotions in the episode were positive anticipation and trust

```{r echo=T}
ggplot(sentiment, aes(sentiment, freq, fill = sentiment)) +
    geom_col(show.legend = FALSE)

```

In the episode the below words had the largest contribution to the overall score for the sentiment observed on the previous graph. We can see that for some of the sentiments there were not even 5 words with frequency higher than 6.  


```{r echo=T}
sentiment %>%
    group_by(sentiment) %>%
    top_n(5) %>%
    ungroup() %>%
    mutate(word = reorder(word, freq)) %>%
    ggplot(aes(freq, word, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~sentiment, scales = "free_y") +
    labs(x = NULL,
         y = NULL,
         title = "Top words contributing to each sentiment")

```

To calculate the overall sentiment score using the NRC dictionary, the data was manipulated: to all words with positive connotation (like joy, surprise) were assigned "positive" value, all words with negative connotations (like fear, disgust) were assigned "negative" value. Lastly, anticipation was assigned neutral value as anticipation can be related to the expectations of something positive or negative. Negative value for each word: freq*(-1), positive value: freq, neutral value: 0. </br> </br>

The overall sentiment score was positive, which means the episode was perceived in a positive way.  


```{r echo=T}
# Overall sentiment: (positive: +1, negative: -1, neutral: 0)
overall_sentiment <- sentiment %>% 
    mutate(sentiment = recode(sentiment, "fear" = "negative", "disgust" = "negative", "anger" = "negative","sadness"="negative", "joy"="positive","surprise"="positive","trust"="positive","anticipation"="neutral" )) %>% 
    mutate(value =
               case_when(sentiment == "negative" ~ freq*(-1), 
                         sentiment == "positive" ~ freq,
                         sentiment == "neutral" ~ 0)) %>%
    group_by(sentiment) %>% 
    summarise(sum_sentiment = sum(value),
              .groups = 'drop')%>%
    as.data.frame()

overall_sentiment

sum(overall_sentiment$sum_sentiment)
```


To validate the results similar data transformation was performed using Bing sentiment dictionary.The difference between sum of negative sentiment and sum of positive sentiment is lower when Bing dictionary was use. However, the overall conclusion remains the same. 

```{r echo=T}
overall_sentiment_bing<-dtm %>% inner_join(get_sentiments("bing")) %>%
    mutate(value =
               case_when(sentiment == "negative" ~ freq*(-1), 
                         sentiment == "positive" ~ freq,
                         sentiment == "neutral" ~ 0)) %>%
    group_by(sentiment) %>% 
    summarise(sum_sentiment = sum(value),
              .groups = 'drop')%>%
    as.data.frame()

overall_sentiment_bing

sum(overall_sentiment_bing$sum_sentiment)
```


Last part of the project examines the change in sentiment throughout the episode. To perform such analysis the words were kept in order of appearance in the transcript and then were split into groups containing about 20 elements in each. When data was not divisible without a remainder by 20, the group size was increased evenly through each group. The episode is perceived positively. Based on the graph it can be observed that the episode starts slightly positive, then a mixture of all sentiments, series of negative peeks and positive peeks. Closer to the end of the episode neutral (anticipation) raises, most probably resulting in hooking the viewer and increased audience retention.  


```{r echo=T, message=FALSE, warning=FALSE, results='hide' }
dtm <- kards.show.analysis.all(name)

```



```{r echo=T, message=FALSE, warning=FALSE}

sentiment.order.kept <- get_sentiments("nrc") %>% 
    right_join(dtm) %>% 
    na.omit() %>%
    ungroup()%>%
    mutate(id = row_number())

# Determine number of groups and leftover rows
n_groups <- ceiling(nrow(sentiment.order.kept) / 20)
n_leftover <- n_groups * 20 - nrow(sentiment.order.kept)

# Assign a number between 1 and n_groups+1 to each row
sentiment.order.kept <- sentiment.order.kept %>%
    mutate(group = rep(1:n_groups, each = 20)[1:nrow(sentiment.order.kept)]) %>%
    mutate(group = ifelse(row_number() > n_groups * 20 - n_leftover, n_groups + 1, group))

# Assign a number between 1 and n_groups to the leftover rows
if (n_leftover > 0) {
    sentiment.order.kept %>%
        tail(n_leftover) %>%
        mutate(group = n_groups + 1) %>%
        bind_rows(head(sentiment.order.kept, nrow(sentiment.order.kept) - n_leftover)) ->
        sentiment.order.kept
}
sentiment.order.kept%>% 
    mutate(
        sentiment = recode(sentiment, 
                           "fear" = "negative", "disgust" = "negative", 
                           "anger"="negative","sadness"="negative",
                           "joy"="positive","surprise"="positive","trust"="positive","anticipation"="neutral" )) %>% 
    group_by(sentiment, group) %>%
    summarise(num_words = n_distinct(word)) %>%
    arrange(sentiment, group)%>%
    filter(sentiment == c("positive", "negative", "neutral"))%>%
    ggplot(aes(group, num_words, colour = sentiment)) + geom_line()+
    scale_color_manual(values=c("#fc413a","#d18c15", "#2da612"))+
    labs(x = "time of the episode",
         y = "number of words in sentiment",
         title = "The changes in sentiment as episode progresses")+
  theme(axis.text.x=element_blank(), 
        axis.ticks.x=element_blank(),
        axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5)
        )



```

```{r echo=T}

dev.off()
```


## Evaluation 

The first thing which is noticed when analyzing the sentiment of the words used in this specific episode, is that prank has negative sentiment based on NRC dictionary. This is interesting because from the perspective of the pranked person this might be negative, but from the audience perspective it might not be the case. </br>
The data sample when filtered by frequency higher than six and only words in NRC dictionary, is rather small. However, two dictionaries were used to check the validity of the results. 



## Summary 
The aim was obtained as one can simply substitute the name of the file and analyse a completely different episode of the show. The project evaluated extend of existence of each sentiment in the episode, sentiment of the whole episode and utilized multiple sentiment dictionaries. Ten NRC sentiments were present in the episode


